# Hadoop Setup and Basic Commands

This guide provides instructions for setting up Hadoop and using basic Hadoop commands.

## Prerequisites

- Java 8 or higher installed on your system.
- At least 4 GB of RAM recommended.
- Sufficient disk space for storing data and logs.

## Installing Java, Creating a User, and Managing Permissions

This section guides you through downloading and installing Java, creating a separate user, giving necessary permissions, and switching to that user.

### Step 1: Download and Install Java

1. **Update the package index:**

   ```bash
   sudo apt update # Fetches the list of available updates
   sudo apt full-upgrade -y # Installs updates; may also remove some packages, if needed
   sudo apt autoremove # Removes any old packages that are no longer needed
   ```
2. **Install OpenJDK (Java Development Kit):**

   ```bash
   sudo apt install openjdk-11-jdk -y
   ```

   You can check the installed Java version with:

   ```bash
   java -version
   ```

   If you prefer a specific version, replace `openjdk-11-jdk` with the appropriate version.
3. **Set up JAVA_HOME environment variable:**

   Add the following lines to your `~/.bashrc`  file:

   ```bash
   nano ~/.bashrc
   ```

   ```bash
   export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))
   export PATH=$PATH:$JAVA_HOME/bin
   ```

   Apply the changes:

   ```bash
   source ~/.bashrc
   ```

### Step 2: Create a New User

1. **Create a new user:**

   Replace `<username>` with the desired username:

   ```bash
   sudo adduser <username>
   ```

   Follow the prompts to set a password and user details.
2. **Assign the user to a specific group (optional):**

   To add the user to a specific group, replace `<groupname>` with the desired group:

   ```bash
   sudo usermod -aG <groupname> <username>
   ```

### Step 3: Assign Permissions

1. **Give the user ownership of a directory:**

   Replace `<directory>` with the path to the directory:

   ```bash
   sudo chown -R <username>:<username> <directory>
   ```

   Example:

   ```bash
   sudo chown -R <username>:<username> /usr/local/hadoop
   ```
2. **Modify directory permissions:**

   You can also set specific permissions for the user:

   ```bash
   sudo chmod 755 <directory>
   ```

### Step 4: Switch to the New User

1. **Switch to the new user:**

   Replace `<username>` with the username created:

   ```bash
   su - <username>
   ```
2. **Verify the switch:**

   You can check the current user with:

   ```bash
   whoami
   ```

### Step 5: Grant Sudo Privileges (Optional)

1. **Edit the sudoers file:**

   ```bash
   sudo visudo
   ```
2. **Add the following line to give the user sudo privileges:**

   Replace `<username>` with the username:

   ```bash
   <username> ALL=(ALL) NOPASSWD:ALL
   ```
3. **Save and exit the editor.** (In `visudo`, press `Ctrl + X`, then `Y`, then `Enter` to save and exit.)

---

With these steps, you've successfully installed Java, created a new user, assigned permissions, and switched to that user. This setup ensures that your Hadoop or other Java-based applications can be managed securely under a dedicated user account.

## Step 1: Download and Install Hadoop

1. **Download Hadoop:**

   Download the latest stable release of Hadoop from the Apache Hadoop website:

   ```bash
   wget https://downloads.apache.org/hadoop/common/hadoop-<version>/hadoop-<version>.tar.gz
   ```

   Replace `<version>` with the version number you want to download (e.g., `3.4.0`).
2. **Extract Hadoop:**

   ```bash
   tar -xzvf hadoop-<version>.tar.gz
   ```
3. **Move Hadoop to `/usr/local/hadoop`:**

   ```bash
   sudo mv hadoop-<version> /usr/local/hadoop
   ```
4. **Check if moved, add directory to store system logs**

   ```bash
   ls /usr/local/hadoop
   sudo mkdir /usr/loacl/hadoop/logs
   ```
5. **Change ownership**

   ```bash
   sudo chown -R hadoop:hadoop /usr/local/hadoop
   ```
6. **Configure environment variables:**

   Add the following lines to your ` ~/.bashrc` file:

   ```bash
   nano ~/.bashrc
   ```

   ```bash
   export HADOOP_HOME=/usr/local/hadoop
   # export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
   export HADOOP_INSTALL=$HADOOP_HOME
   export HADOOP_MAPRED_HOME=$HADOOP_HOME 
   export HADOOP_COMMON_HOME=$HADOOP_HOME
   export HADOOP_HDFS_HOME=$HADOOP_HOME
   export YARN_HOME=$HADOOP_HOME
   export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native 
   export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin 
   export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
   ```

   Apply the changes:
   `Ctrl + O` to save, press `Enter`, `Ctrl + X` to exit
7. check path to java compiler, determine the open JDK location and environment variables

   ```bash
   which javac
   readlink -f  

   echo $HADOOP_HOME 
   ```

## Step 2: Configure Hadoop

```bash
     cd $HADOOP_HOME
     ls
```

1. **Edit `hadoop-env.sh`:**

   Open the `hadoop-env.sh` file:

   ```bash
   nano $HADOOP_HOME/etc/hadoop/hadoop-env.sh
   ```

   Set the `JAVA_HOME` path:

   ```bash
   export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
   export HADOOP_CLASSPATH+=" $HADOOP_HOME/lib/*.jar"
   ```

   Apply the changes:
   `Ctrl + O` to save, press `Enter`, `Ctrl + X` to exit

   for `javax activation jar`

   ```bash
   cd /usr/local/hadoop/lib
   ```

   ```bash
   sudo wget https://jcenter.bintray.com/javax/activation/javax.activation-api/1.2.0/javax.activation-api-1.2.0.jar
   ```

- check hadoop version

```bash
hadoop version
```

- check for necessary configuaration files

```bash
ls $HADOOP_HOME/etc/hadoop/
```

2. **Configure core-site.xml:**

   Open `core-site.xml`:

   ```bash
   nano $HADOOP_HOME/etc/hadoop/core-site.xml
   ```

   Add the following configuration:

   ```xml
     <configuration>
          <property>
               <name>fs.defaultFS</name>
               <value>hdfs://localhost:9000</value>
               <description>The default file system URI</description>
          </property>
     </configuration>
   ```

- create a directory for storing node metadata and change the ownership to hadoop

```bash
sudo mkdir -p /home/hadoop/hdfs/{namenode,datanode}
sudo chown -R hadoop:hadoop /home/hadoop/hdfs
```

```bash
ls
ls hdfs
```

3. **Configure hdfs-site.xml:**

   Open `hdfs-site.xml`:

   ```bash
   nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
   ```

   Add the following configuration:

   ```xml
   <configuration>
        <property>
             <name>dfs.replication</name>
             <value>1</value>
        </property>
        <property>
             <name>dfs.namenode.name.dir</name>
             <value>file:///home/hadoop/hdfs/namenode</value>
        </property>
        <property>
             <name>dfs.datanode.data.dir</name>
             <value>file:///home/hadoop/hdfs/datanode</value>
        </property>
        <property>
             <name>dfs.namenode.http-address</name>
             <value>localhost:9870</value>
        </property>
        <property>
             <name>dfs.namenode.rpc-address</name>
             <value>localhost:9000</value>
        </property>
   </configuration>
   ```
4. **Configure mapred-site.xml:**

   Copy the template file:

   ```bash
   cp $HADOOP_HOME/etc/hadoop/mapred-site.xml.template $HADOOP_HOME/etc/hadoop/mapred-site.xml
   ```

   Open `mapred-site.xml`:

   ```bash
   nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
   ```

   Add the following configuration:

   ```xml
   <configuration>
       <property>
           <name>mapreduce.framework.name</name>
           <value>yarn</value>
       </property>
   </configuration>
   ```
5. **Configure yarn-site.xml:**

   Open `yarn-site.xml`:

   ```bash
   nano $HADOOP_HOME/etc/hadoop/yarn-site.xml
   ```

   Add the following configuration:

   ```xml
   <configuration>
       <property>
           <name>yarn.nodemanager.aux-services</name>
           <value>mapreduce_shuffle</value>
       </property>
   </configuration>
   ```

## Step 3: Format the Namenode

Before starting Hadoop, format the namenode:
Validate Hadoop Configuration. 
- wipes out all existing data in the Namenode, effectively setting up a new HDFS file system. 
- As a result, it also deletes the file system metadata, including files, directories, and block locations

```bash
hdfs namenode -format
```

# Setting Up SSH on Localhost and Starting Hadoop Services

This guide provides steps to set up SSH on your localhost, generate SSH keys, and start Hadoop services.

## Step 1: Install OpenSSH Server and Client

First, install the necessary OpenSSH packages:

```bash
sudo apt install openssh-server openssh-client -y
```

## Step 2: Verify SSH Installation
Check the SSH version to verify the installation:

```bash
ssh -V
```
## Step 3: Start and Check SSH Service
Start the SSH service:

```bash
sudo service ssh start
```
Check the status of the SSH service:

```bash
sudo service ssh status
```
The status should indicate that SSH is active and running.

## Step 4: Generate SSH Keys
Generate an RSA key pair:

```bash
ssh-keygen -t rsa -P ""
```
This generates a pair of SSH keys without a passphrase.

Add the public key to authorized_keys:

```bash
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```
Set the appropriate permissions:

```bash
chmod 700 ~/.ssh
chmod 600 ~/.ssh/authorized_keys
```
## Step 5: Allow SSH Through the Firewall
If you have UFW (Uncomplicated Firewall) enabled, allow SSH connections:

```bash
sudo ufw allow ssh
```
## Step 6: Test SSH Connection to Localhost
Test the SSH setup by connecting to localhost:

```bash
ssh localhost
```
If everything is set up correctly, you should be able to SSH into your localhost without a password prompt.

## Step 7: Start Hadoop Services
Once SSH is configured and tested, you can start Hadoop services:

Start the HDFS services:

```bash
start-dfs.sh
```
Start the YARN services:

```bash
start-yarn.sh
```
Check the running Java processes (Hadoop daemons):

```bash
jps
```
The output should list Hadoop-related processes such as NameNode, DataNode, ResourceManager, and NodeManager.

## Step 8: Verify Hadoop Services in Web Interface

### 1. Ensure Hadoop Services are Running

Before checking the web interfaces, make sure that Hadoop services are started. You can start the HDFS and YARN services with the following commands:

```bash
start-dfs.sh
start-yarn.sh
```
### 2. Access Hadoop Web Interfaces
Once the services are running, you can access the web interfaces for HDFS and YARN:

1. HDFS Web Interface
- NameNode UI: Provides information about the HDFS cluster, including the file system hierarchy and health of the NameNode.

Access it by navigating to:

```plaintext
http://localhost:9870
```

- DataNode UI: Shows information about individual DataNodes, including their status and data stored.

Access it by navigating to:

```plaintext
http://localhost:9864
```
2. YARN Web Interface
- ResourceManager UI: Displays information about the YARN ResourceManager, including running applications, cluster resources, and nodes.

Access it by navigating to:

```plaintext
http://localhost:8088
```
- NodeManager UI: Shows information about the individual NodeManagers in the YARN cluster.

Access it by navigating to:

```plaintext
http://localhost:8042
```

## Step 9: Stop Hadoop Services and Exit SSH

To stop the HDFS and YARN services, use the following commands:

- **Stop HDFS services:**

     ```bash
     stop-dfs.sh
     ```
- **Stop YARN services:**

     ```bash
     stop-yarn.sh
     ```
- **Exit SSH Session:**

     ```bash
     exit
     ```


# Hadoop Commands Guide

This guide provides a comprehensive list of common Hadoop commands, along with their descriptions and usage examples.

## 1. HDFS Commands

### Create a Directory in HDFS

Create a new directory in HDFS:

```bash
hdfs dfs -mkdir /new_directory
```

### List Files in HDFS

List files and directories in HDFS:

```bash
hdfs dfs -ls /
```

### Upload Files to HDFS

Upload files from the local filesystem to HDFS:

```bash
hdfs dfs -put /local/path/to/file /hdfs/path/
```

### Download Files from HDFS

Download files from HDFS to the local filesystem:

```bash
hdfs dfs -get /hdfs/path/to/file /local/path/
```

### Delete Files or Directories in HDFS

Delete files or directories from HDFS:

```bash
hdfs dfs -rm /hdfs/path/to/file
```

To delete a directory and its contents:

```bash
hdfs dfs -rm -r /hdfs/path/to/directory
```

## 3. YARN Commands

### Check YARN Application Status

List all applications running in YARN:

```bash
yarn application -list
```

To check the status of a specific application:

```bash
yarn application -status <application_id>
```

## 4. Hadoop Job Management

### Submit a MapReduce Job

Submit a Hadoop MapReduce job:

```bash
hadoop jar /path/to/job.jar input_directory output_directory
```

### Check Running Jobs

To check currently running jobs:

```bash
hadoop job -list
```

### Kill a Running Job

To kill a specific running job:

```bash
hadoop job -kill <job_id>
```

### Access Web Interfaces

- **NameNode:** [http://localhost:9870](http://localhost:9870)
- **DataNode:** [http://localhost:9864](http://localhost:9864)
- **ResourceManager:** [http://localhost:8088](http://localhost:8088)
- **NodeManager:** [http://localhost:8042](http://localhost:8042)


## 5. Managing Hadoop Logs

### View Hadoop Logs

To view the logs for NameNode, DataNode, ResourceManager, or NodeManager:

```bash
tail -f $HADOOP_HOME/logs/<log_file_name>
```

## 8. Miscellaneous Commands

### Check HDFS Disk Usage

To check the disk usage of HDFS:

```bash
hdfs dfs -du -h /
```

### HDFS Safe Mode

- Enter safe mode:

  ```bash
  hdfs dfsadmin -safemode enter
  ```
- Leave safe mode:

  ```bash
  hdfs dfsadmin -safemode leave
  ```

