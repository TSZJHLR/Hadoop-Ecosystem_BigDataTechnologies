# Hadoop Operations with Netflix Dataset

## Step 1: Login to Hadoop User

```bash
su - hadoop
```

## Step 2: Start SSH Service on Localhost

```bash
sudo service ssh start
```

## Step 3: Start HDFS, YARN, HBase, Thrift, and Zookeeper Services

```bash
start-dfs.sh
start-yarn.sh
start-hbase.sh
/usr/local/HBase/bin/hbase-daemon.sh start thrift -p 9090
/usr/local/zookeeper/bin/zkServer.sh start
jps
```

## Step 4: Copy Dataset from Windows to Linux

cp /mnt/`<path/to/dataset-location/in-windows>`/netflix.csv .

## Step 5: Remove Old HDFS Directory, Create New, and Put Dataset into It

```bash
hadoop fs -rm -r /user/hadoop/netflix
hadoop fs -mkdir /user/hadoop/netflix
hadoop fs -put /home/hadoop/netflix.csv /user/hadoop/netflix
```

## Step 6: Execute 5 Hive Queries

```bash
hive
```


```sql
CREATE DATABASE IF NOT EXISTS netflix_db;
USE netflix_db;
CREATE TABLE IF NOT EXISTS netflix_data (
    show_id STRING,
    type STRING,
    title STRING,
    director STRING,
    cast STRING,
    country STRING,
    date_added STRING,
    release_year INT,
    rating STRING,
    duration STRING,
    listed_in STRING,
    description STRING
) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' STORED AS TEXTFILE;

LOAD DATA INPATH '/user/hadoop/netflix/netflix.csv' INTO TABLE netflix_data;

SELECT type, COUNT(*) FROM netflix_data GROUP BY type;
SELECT country, COUNT(*) FROM netflix_data WHERE type='Movie' GROUP BY country ORDER BY COUNT(*) DESC LIMIT 5;
```

```sql
show databases;
```
```sql
show tables;
```
```sql
describe netfilx_data;
```
```sql

```

## Step 7: Execute 5 Pig Queries

```bash
pig
```

```pig
movies = LOAD '/user/hadoop/netflix/netflix.csv' USING PigStorage(',') AS 
    (show_id:chararray, type:chararray, title:chararray, director:chararray, cast:chararray, country:chararray, date_added:chararray, release_year:int, rating:chararray, duration:chararray, listed_in:chararray, description:chararray);

grouped_by_type = GROUP movies BY type;
movie_counts = FOREACH grouped_by_type GENERATE group, COUNT(movies);
DUMP movie_counts;

grouped_by_country = GROUP movies BY country;
country_counts = FOREACH grouped_by_country GENERATE group, COUNT(movies);
DUMP country_counts;

recent_movies = FILTER movies BY release_year > 2020;
DUMP recent_movies;
```

## Step 8: Execute 5 HBase Commands (Automated Insertion with HappyBase)

### 

### Python Script for Automated Insertion

Create a file `insert_data.py`:

```python
import happybase
import csv

connection = happybase.Connection('localhost')
table = connection.table('netflix_titles')

with open('/home/hadoop/netflix.csv', 'r') as file:
    for line in file:
        fields = line.strip().split(',')
        row_key = fields[0]
        table.put(row_key, {
            b'title:type': fields[1],
            b'title:title': fields[2],
            b'title:director': fields[3],
            b'title:cast': fields[4],
            b'title:country': fields[5],
            b'title:date_added': fields[6],
            b'title:release_year': fields[7],
            b'title:rating': fields[8],
            b'title:duration': fields[9],
            b'title:listed_in': fields[10],
            b'title:description': fields[11],
        })

print("Data insertion completed.")
```

Run the script:

```bash
python3 insert_data.py
```

```shell
list
count 'netflix_data'
get 'netflix_data', 'row#'
describe 'netflix_data'
scan 'netflix_data'
delete
disable
drop
```


## Step 9: List MapReduce Jobs and Other Ecosystem Jobs on Localhost

### Access Web Interfaces:

```bash
http://localhost:9870  # HDFS NameNode
http://localhost:8088  # YARN ResourceManager
http://localhost:16910 # HBase Master
http://localhost:10000 # HiveServer2 via Beeline
```

## Step 10: Shutdown Services and Wrap Up

### Stop HBase Thrift, YARN, DFS, and Check JPS

```bash
/usr/local/HBase/bin/hbase-daemon.sh stop thrift
stop-hbase.sh
stop-yarn.sh
stop-dfs.sh
jps
```

### Exit Localhost

```bash
exit
```

### Wrap Up

```bash
logout
```
