# Map Reduce Job
## Switch to the New User

   Replace `<username>` with the username created for hadoop:

   ```bash
   su - <username>
   ```

**search for output:**

```bash
export HADOOP_CLASSPATH=$(hadoop classpath)
export $HADOOP_CLASSPATH
```
- copy element content up to yarn/*

## **Configure mapred-site.xml:**

   Open `mapred-site.xml`:

```bash
   nano $HADOOP_HOME/etc/hadoop/mapred-site.xml
```

   Add the following configuration:

```xml
<configuration>
          <property>
               <name>mapreduce.framework.name</name>
               <value>yarn</value>
           </property>
           <property>
               <name>yarn.app.mapreduce.am.env</name>
               <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
          </property>
          <property>
               <name>mapreduce.map.env</name>
               <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
          </property>
          <property>
               <name>mapreduce.reduce.env</name>
               <value>HADOOP_MAPRED_HOME=/usr/local/hadoop</value>
          </property>
          <property>
               <name>mapreduce.application.classpath</name>
               <value>/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:
               /usr/local/hadoop/share/hadoopcommon/*:/usr/local/hadoop/share/hadoop/hdfs:
               /usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:
               /usr/local/hadoop/share/hadoop/mapreduce/*:/usr/local/hadoop/share/hadoop/yarn:
               /usr/local/hadoop/share/hadoop/yarn/lib/*/usr/local/hadoop/share/hadoop/yarn/*</value>
          </property>
</configuration>
```

## Start SSH Service and LocalHost:

```bash
sudo service ssh start
```
```bash
ssh localhost
```
## Start Hadoop Services
Start the HDFS services:
```bash
start-dfs.sh
```
Start the YARN services:
```bash
start-yarn.sh
```
Check the running Java processes (Hadoop daemons):
```bash
jps
```
## Configuring and Running a MapReduce Job

This guide provides detailed instructions on configuring and running a MapReduce job in Hadoop.

## 1. Prerequisites

Before running a MapReduce job, ensure that the following prerequisites are met:

- **Hadoop is installed and configured**: Hadoop should be installed and properly configured on your cluster.
- **Java is installed**: MapReduce requires Java to be installed on your system.
- **HDFS is running**: Ensure that the HDFS services are started.

## 2. Writing a MapReduce Job

A MapReduce job typically consists of three main classes: Mapper, Reducer, and Driver.

### Example Java MapReduce Code

Hereâ€™s a simple example of a Word Count MapReduce job in Java:

**WordCount.java**

```java
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

public class WordCount {

    public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable> {
        private final static IntWritable one = new IntWritable(1);
        private Text word = new Text();

        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            String[] words = value.toString().split("\\s+");
            for (String str : words) {
                word.set(str);
                context.write(word, one);
            }
        }
    }

    public static class IntSumReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
        private IntWritable result = new IntWritable();

        public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {
            int sum = 0;
            for (IntWritable val : values) {
                sum += val.get();
            }
            result.set(sum);
            context.write(key, result);
        }
    }

    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        Job job = Job.getInstance(conf, "word count");
        job.setJarByClass(WordCount.class);
        job.setMapperClass(TokenizerMapper.class);
        job.setCombinerClass(IntSumReducer.class);
        job.setReducerClass(IntSumReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        FileInputFormat.addInputPath(job, new Path(args[0]));
        FileOutputFormat.setOutputPath(job, new Path(args[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
}
```

## 3. Compile and Package the MapReduce Job

1. **Compile the Java Code**:

   ```bash
   javac -classpath `hadoop classpath` -d /path/to/output WordCount.java
   ```

2. **Create a JAR File**:

   ```bash
   jar cf wordcount.jar -C /path/to/output .
   ```

## 4. Upload Input Data to HDFS

Upload the input data file to HDFS:

```bash
hdfs dfs -mkdir /input
hdfs dfs -put /local/path/to/input/file /input/
```

## 5. Run the MapReduce Job

Submit the MapReduce job to Hadoop:

```bash
hadoop jar /path/to/wordcount.jar WordCount /input /output
```

- `/input` is the HDFS directory containing the input data.
- `/output` is the HDFS directory where the output will be written.

## 6. Monitor Job Progress

You can monitor the progress of your MapReduce job through the ResourceManager web UI:

- **ResourceManager UI:** [http://localhost:8088](http://localhost:8088)

## 7. Retrieve the Output

After the job completes, retrieve the output from HDFS:

```bash
hdfs dfs -ls /output
hdfs dfs -cat /output/part-r-00000
```

- `part-r-00000` is the default output file name for the reducer output.

## 8. Clean Up

Remove the input and output directories from HDFS if no longer needed:

```bash
hdfs dfs -rm -r /input
hdfs dfs -rm -r /output
```


# check for file to run mapreduce

```bash
$HADOOP_HOME/bin/hadoop fs -ls /
```

```bash
hadoop fs -put test.txt /test
```

```bash
hdfs dfsadmin -report
```

```bash
hadoop fs -put mysales.txt /sujal
```

```bash
hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.4.0.jar wordcount /sujal/mysales.txt /output/part
```
